{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e4c2941",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T19:43:23.211434Z",
     "iopub.status.busy": "2022-09-16T19:43:23.210913Z",
     "iopub.status.idle": "2022-09-16T19:43:23.216771Z",
     "shell.execute_reply": "2022-09-16T19:43:23.215760Z"
    },
    "papermill": {
     "duration": 0.028194,
     "end_time": "2022-09-16T19:43:23.225962",
     "exception": false,
     "start_time": "2022-09-16T19:43:23.197768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#training methods:\n",
    "#-> normal classification\n",
    "#-> arc classification\n",
    "#-> encoding\n",
    "#-> trained embedding\n",
    "#-> trained retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36c9e1f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T19:43:23.246451Z",
     "iopub.status.busy": "2022-09-16T19:43:23.245991Z",
     "iopub.status.idle": "2022-09-16T19:43:24.257148Z",
     "shell.execute_reply": "2022-09-16T19:43:24.255974Z"
    },
    "papermill": {
     "duration": 1.023783,
     "end_time": "2022-09-16T19:43:24.259700",
     "exception": false,
     "start_time": "2022-09-16T19:43:23.235917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture-dataset  fruits360        imagenet1000  products-10k  shopee\r\n",
      "food101\t\t      imagenet-sketch  places\t     rp2k\r\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/input/embedding-data/embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bc766f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T19:43:24.274815Z",
     "iopub.status.busy": "2022-09-16T19:43:24.274429Z",
     "iopub.status.idle": "2022-09-16T19:43:26.432389Z",
     "shell.execute_reply": "2022-09-16T19:43:26.431428Z"
    },
    "papermill": {
     "duration": 2.167744,
     "end_time": "2022-09-16T19:43:26.434368",
     "exception": false,
     "start_time": "2022-09-16T19:43:24.266624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "from torch.nn import LayerNorm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d838a6d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T19:43:26.449881Z",
     "iopub.status.busy": "2022-09-16T19:43:26.449381Z",
     "iopub.status.idle": "2022-09-16T19:43:26.460682Z",
     "shell.execute_reply": "2022-09-16T19:43:26.459699Z"
    },
    "papermill": {
     "duration": 0.021629,
     "end_time": "2022-09-16T19:43:26.462768",
     "exception": false,
     "start_time": "2022-09-16T19:43:26.441139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ArcMarginProduct(nn.Module):\n",
    "    r\"\"\"Implement of large margin arc distance: :\n",
    "        Args:\n",
    "            in_features: size of each input sample\n",
    "            out_features: size of each output sample\n",
    "            s: norm of input feature\n",
    "            m: margin\n",
    "            cos(theta + m)\n",
    "        \"\"\"\n",
    "    def __init__(self, in_features, out_features, s=30.0, \n",
    "                 m=0.5, easy_margin=False, ls_eps=0.0):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps  # label smoothing\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
    "        one_hot = torch.zeros(cosine.size(), device=device)\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) ------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03ce8cd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T19:43:26.477722Z",
     "iopub.status.busy": "2022-09-16T19:43:26.476945Z",
     "iopub.status.idle": "2022-09-16T19:43:29.365872Z",
     "shell.execute_reply": "2022-09-16T19:43:29.364977Z"
    },
    "papermill": {
     "duration": 2.898565,
     "end_time": "2022-09-16T19:43:29.367940",
     "exception": false,
     "start_time": "2022-09-16T19:43:26.469375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TopTrainer(\n",
       "  (fw): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=64, bias=True)\n",
       "  )\n",
       "  (pool): AdaptiveAvgPool1d(output_size=64)\n",
       "  (arc): ArcMarginProduct()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TopTrainer(nn.Module):\n",
    "    def __init__(self, fw):\n",
    "        super().__init__()\n",
    "        self.fw = fw\n",
    "        self.lid = None\n",
    "        self.pool = nn.AdaptiveAvgPool1d(64)\n",
    "        self.arc = ArcMarginProduct(1000,1000)\n",
    "    def arc_sim(self, x, label):\n",
    "        x = self.fw(x)\n",
    "        return self.arc( x, label)\n",
    "    def with_arc(self, x, label, use_lid=True):\n",
    "        x = self.fw(x)\n",
    "        if use_lid:\n",
    "            x = self.lid(x)\n",
    "        x = self.arc( x, label)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x, use_lid=True):\n",
    "        x = self.fw(x) \n",
    "        if use_lid:\n",
    "            x = self.lid(x)\n",
    "        return x\n",
    "\n",
    "model = TopTrainer(\n",
    "    nn.Sequential(\n",
    "            nn.Linear( 768, 64)\n",
    "        )\n",
    ").to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23a75d70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T19:43:29.385381Z",
     "iopub.status.busy": "2022-09-16T19:43:29.384700Z",
     "iopub.status.idle": "2022-09-16T19:43:29.396513Z",
     "shell.execute_reply": "2022-09-16T19:43:29.395686Z"
    },
    "papermill": {
     "duration": 0.023312,
     "end_time": "2022-09-16T19:43:29.398451",
     "exception": false,
     "start_time": "2022-09-16T19:43:29.375139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class class_ds_config:\n",
    "    def __init__(\n",
    "            self,\n",
    "            path,\n",
    "            lr,\n",
    "            epochs,\n",
    "            train, \n",
    "            max_class\n",
    "    ):\n",
    "        self.path=path\n",
    "        self.lr=lr\n",
    "        self.epochs=epochs\n",
    "        self.train=train\n",
    "        self.max_class = max_class\n",
    "class embed_ds_config:\n",
    "    def __init__(\n",
    "        self,\n",
    "        path,\n",
    "        lr,\n",
    "        epochs,\n",
    "        train,\n",
    "        criterion\n",
    "    ):\n",
    "        self.path=path\n",
    "        self.lr=lr\n",
    "        self.epochs=epochs\n",
    "        self.train=train,\n",
    "        self.criterion=criterion\n",
    "class config:\n",
    "    \n",
    "    class_data_ls = [     \n",
    "        class_ds_config(\n",
    "            '/kaggle/input/embedding-data/embeddings/architecture-dataset',\n",
    "            0.0001, 2, True, 25\n",
    "        ),\n",
    "        class_ds_config(\n",
    "            '/kaggle/input/notebook-data/classification/GPR',\n",
    "            0.00001,2,True,1200\n",
    "        ),\n",
    "        class_ds_config(\n",
    "            '/kaggle/input/embedding-data/embeddings/food101',\n",
    "            0.00008, 2, True, 101\n",
    "        ),\n",
    "        class_ds_config(\n",
    "            '/kaggle/input/embedding-data/embeddings/fruits360',\n",
    "            0.0001, 1, False, 131\n",
    "        ),\n",
    "        class_ds_config(\n",
    "            '/kaggle/input/embedding-data/embeddings/rp2k',\n",
    "            0.0001, 15, True, 2384\n",
    "        ),\n",
    "        class_ds_config(\n",
    "            '/kaggle/input/embedding-data/embeddings/products-10k',\n",
    "            0.0001, 5, True, 9691\n",
    "        ),\n",
    "        class_ds_config(\n",
    "            '/kaggle/input/embedding-data/embeddings/shopee',\n",
    "            0.0001, 10, True, 11014\n",
    "        ),\n",
    "        class_ds_config(\n",
    "            '/kaggle/input/embedding-data/embeddings/imagenet1000',\n",
    "            0.0001, 10, True, 1000\n",
    "        ),\n",
    "        class_ds_config(\n",
    "            '/kaggle/input/embedding-data/embeddings/places',\n",
    "            0.0001, 2, True, 1000\n",
    "        ),\n",
    "        \n",
    "        class_ds_config(\n",
    "            '/kaggle/input/embedding-data/embeddings/imagenet-sketch',\n",
    "            0.0001, 2, True, 1000\n",
    "        ),\n",
    "        class_ds_config(\n",
    "            '/kaggle/input/notebook-data/classification/imagenet1000',\n",
    "            0.0001,3,False,1000\n",
    "        )\n",
    "    ]\n",
    "    embed_data_ls = [\n",
    "        embed_ds_config(\n",
    "            '/kaggle/input/notebook-data/embedding/imagenet1000',\n",
    "            0.0005,\n",
    "            3,\n",
    "            False,\n",
    "            nn.MSELoss()\n",
    "        ),\n",
    "        embed_ds_config(\n",
    "            '/kaggle/input/notebook-data/embedding/google-landmarks-2021-V1',\n",
    "            0.0005,\n",
    "            2,\n",
    "            False,\n",
    "            nn.MSELoss()\n",
    "        ),\n",
    "        embed_ds_config(\n",
    "            '/kaggle/input/notebook-data/embedding/fashion',\n",
    "            0.00005,\n",
    "            2,\n",
    "            False,\n",
    "            nn.MSELoss()\n",
    "        ),\n",
    "        embed_ds_config(\n",
    "            '/kaggle/input/notebook-data/embedding/GPR12000',\n",
    "            0.0005,\n",
    "            2,\n",
    "            False,\n",
    "            nn.MSELoss()\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e08de53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T19:43:29.412947Z",
     "iopub.status.busy": "2022-09-16T19:43:29.412688Z",
     "iopub.status.idle": "2022-09-16T19:43:29.425893Z",
     "shell.execute_reply": "2022-09-16T19:43:29.424910Z"
    },
    "papermill": {
     "duration": 0.023074,
     "end_time": "2022-09-16T19:43:29.428216",
     "exception": false,
     "start_time": "2022-09-16T19:43:29.405142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieval_evaluate( model, ds, ds_config):\n",
    "    def get_embeds( model, ds):\n",
    "        embeds = []\n",
    "        labels = []\n",
    "        with torch.no_grad():\n",
    "            for vec, labels_ in ds:\n",
    "                out = model( vec.to(device), use_lid=False)\n",
    "                embeds.append(out)\n",
    "                labels.append(labels_)\n",
    "        embeds = torch.cat(embeds)\n",
    "        labels = torch.cat(labels)\n",
    "        return (embeds, labels)\n",
    "    def normalize(a, eps=1e-8):\n",
    "        a_n = a.norm(dim=1)[:, None]\n",
    "        a_norm = a / torch.max(a_n, eps * torch.ones_like(a_n))\n",
    "        return a_norm\n",
    "    def k_nearest_neighbors(embeds, k=5):\n",
    "        #print(embeds.shape)\n",
    "        normalized = normalize(embeds)\n",
    "        preds = normalized @ normalized.T\n",
    "        #print(preds.shape)\n",
    "        #vals, indices = preds.sort(dim=1, descending=True)\n",
    "        vals,indices = torch.topk(preds,6)\n",
    "        k += 1\n",
    "        return indices[:, 1:k].long()\n",
    "    embeds, labels = get_embeds( model, ds)\n",
    "    preds = k_nearest_neighbors(embeds, k=5)\n",
    "    accs = (labels[preds] == labels.view(-1, 1)).float().mean(dim=1)\n",
    "    return accs.mean()\n",
    "\n",
    "last_retrieval = [0 for i in config.class_data_ls]\n",
    "def get_retrieval_scores():\n",
    "    idx=0\n",
    "    for ds_config in config.class_data_ls:\n",
    "        \n",
    "        ds = classification_dataset(ds_config)\n",
    "        if ds_config.path.split('/')[-1] == 'products-10k':\n",
    "            #print('splicing ds...',end='    ')\n",
    "            ds = ds.splice(0,512)\n",
    "        #ds = ds.splice(0,512)\n",
    "        #val_score = retrieval_evaluate( model, ds.val_split(), ds_config)\n",
    "        #train_score = retrieval_evaluate( model, ds.train_split(), ds_config)\n",
    "        score = retrieval_evaluate( model, ds, ds_config)\n",
    "        diff = score.item() - last_retrieval[idx]\n",
    "        delta = '+' if diff >= 0 else ' ' \n",
    "        pad = 20 - len(ds_config.path.split('/')[-1])\n",
    "        print( ds_config.path.split('/')[-1], \" \"*pad,\": \", score.item(),\"  {}{}\".format(delta, diff))\n",
    "        last_retrieval[idx] = score.item()\n",
    "        #print( ds_config.path.split('/')[-1],\" train: \", train_score.item())\n",
    "        #print( ds_config.path.split('/')[-1],\"   val: \", val_score.item())\n",
    "        idx+=1\n",
    "def _eval( model):\n",
    "    for ds_config in config.class_data_ls[:1]:\n",
    "        ds = classification_dataset(ds_config)\n",
    "        ds = ds.val_split()\n",
    "        score = retrieval_evaluate( model, ds, ds_config)\n",
    "        print( ds_config.path.split('/')[-1],\": \", score.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3a27bc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T19:43:29.443899Z",
     "iopub.status.busy": "2022-09-16T19:43:29.443156Z",
     "iopub.status.idle": "2022-09-16T19:43:29.453540Z",
     "shell.execute_reply": "2022-09-16T19:43:29.452727Z"
    },
    "papermill": {
     "duration": 0.019826,
     "end_time": "2022-09-16T19:43:29.455475",
     "exception": false,
     "start_time": "2022-09-16T19:43:29.435649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class classification_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.vecs = ['vec/'+_dir for _dir in os.listdir(os.path.join(config.path, 'vec'))]\n",
    "        self.vecs.sort()\n",
    "        self.labels = ['label/'+_dir for _dir in os.listdir(os.path.join(config.path, 'label'))]\n",
    "        self.labels.sort()\n",
    "    def __len__(self):\n",
    "        return len(self.vecs)\n",
    "    def train_split(self):\n",
    "        _len = len(self)\n",
    "        split = int(_len * 0.8)\n",
    "        self.vecs = self.vecs[:split]\n",
    "        self.labels = self.labels[:split]\n",
    "        return self\n",
    "    def val_split(self):\n",
    "        _len = len(self)\n",
    "        split = int(_len * 0.8)\n",
    "        self.vecs = self.vecs[split:]\n",
    "        self.labels = self.labels[split:]\n",
    "        return self\n",
    "        \n",
    "    def splice(self, start, end):\n",
    "        self.vecs = self.vecs[start:end]\n",
    "        self.labels = self.labels[start:end]\n",
    "        return self\n",
    "    def __getitem__(self, idx):\n",
    "        if self.vecs[idx].split('/')[1] != self.labels[idx].split('/')[1]:\n",
    "            print('error')\n",
    "        vec = torch.load(os.path.join(self.config.path,self.vecs[idx]),map_location=device)\n",
    "        label = torch.load(os.path.join(self.config.path,self.labels[idx]),map_location=device)\n",
    "        return vec, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc2af06a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T19:43:29.471916Z",
     "iopub.status.busy": "2022-09-16T19:43:29.470224Z",
     "iopub.status.idle": "2022-09-16T19:43:29.479401Z",
     "shell.execute_reply": "2022-09-16T19:43:29.478517Z"
    },
    "papermill": {
     "duration": 0.019312,
     "end_time": "2022-09-16T19:43:29.481504",
     "exception": false,
     "start_time": "2022-09-16T19:43:29.462192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_classification_ds( model, ds, ds_config):\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=ds_config.lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(ds_config.epochs):\n",
    "        print('training epoch ',epoch+1,'...')\n",
    "        for i, batch in enumerate(ds):\n",
    "            _input, label = batch\n",
    "            \n",
    "            _input = _input.to(device)\n",
    "            optim.zero_grad()\n",
    "            output = model(_input)\n",
    "            \n",
    "            loss = criterion( output, label.to(device))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "    return model\n",
    "\n",
    "def eval_classification_ds( model, ds, ds_config):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    total_loss=0\n",
    "    for i, batch in enumerate(ds):\n",
    "        _input, label = batch\n",
    "        _input = _input.to(device)\n",
    "        output = model(_input)\n",
    "        loss = criterion( output, label.to(device)).detach().item()\n",
    "        total_loss += loss\n",
    "    return total_loss / len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e31d100",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T19:43:29.497277Z",
     "iopub.status.busy": "2022-09-16T19:43:29.496987Z",
     "iopub.status.idle": "2022-09-16T19:43:29.507995Z",
     "shell.execute_reply": "2022-09-16T19:43:29.507154Z"
    },
    "papermill": {
     "duration": 0.021521,
     "end_time": "2022-09-16T19:43:29.510008",
     "exception": false,
     "start_time": "2022-09-16T19:43:29.488487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                e_w = p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "                self.state[p][\"e_w\"] = e_w\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        raise NotImplementedError(\"SAM doesn't work like the other optimizers, you should first call `first_step` and the `second_step`; see the documentation for more info.\")\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        p.grad.norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06dbb618",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T19:43:29.525040Z",
     "iopub.status.busy": "2022-09-16T19:43:29.524282Z",
     "iopub.status.idle": "2022-09-16T20:04:45.427963Z",
     "shell.execute_reply": "2022-09-16T20:04:45.426103Z"
    },
    "papermill": {
     "duration": 1275.913411,
     "end_time": "2022-09-16T20:04:45.430222",
     "exception": false,
     "start_time": "2022-09-16T19:43:29.516811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture-dataset  :  0.7294535040855408   +0.7294535040855408\n",
      "GPR                   :  0.676633358001709   +0.676633358001709\n",
      "food101               :  0.7326732277870178   +0.7326732277870178\n",
      "fruits360             :  0.99440598487854   +0.99440598487854\n",
      "rp2k                  :  0.429698646068573   +0.429698646068573\n",
      "products-10k          :  0.17433473467826843   +0.17433473467826843\n",
      "shopee                :  0.34236496686935425   +0.34236496686935425\n",
      "imagenet1000          :  0.4473622143268585   +0.4473622143268585\n",
      "places                :  0.40837037563323975   +0.40837037563323975\n",
      "imagenet-sketch       :  0.5465935468673706   +0.5465935468673706\n",
      "imagenet1000          :  0.44729891419410706   +0.44729891419410706\n",
      "==================================================\n",
      "pre training loss:  3.253124757607778\n",
      "training dataset: architecture-dataset\n",
      "training epoch  1 ...      884.9143877029419\n",
      "training epoch  2 ...      461.63996028900146\n",
      "post training loss:  3.2141392389933268\n",
      "finished.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "pre training loss:  7.1238253180185955\n",
      "training dataset: GPR\n",
      "training epoch  1 ...      2676.4918718338013\n",
      "training epoch  2 ...      1677.4902620315552\n",
      "post training loss:  7.168773145675659\n",
      "finished.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "pre training loss:  4.659915701943155\n",
      "training dataset: food101\n",
      "training epoch  1 ...      2118.8903737068176\n",
      "training epoch  2 ...      909.3862373828888\n",
      "post training loss:  4.810560445607819\n",
      "finished.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "pre training loss:  7.870825245629909\n",
      "training dataset: rp2k\n",
      "training epoch  1 ...      11009.846053123474\n",
      "training epoch  2 ...      7146.006411552429\n",
      "training epoch  3 ...      6116.006052494049\n",
      "training epoch  4 ...      5700.041705608368\n",
      "training epoch  5 ...      5464.231322288513\n",
      "training epoch  6 ...      5305.244912147522\n",
      "training epoch  7 ...      5185.9332938194275\n",
      "training epoch  8 ...      5090.7793645858765\n",
      "training epoch  9 ...      5011.7022404670715\n",
      "training epoch  10 ...      4944.0699582099915\n",
      "training epoch  11 ...      4885.0422258377075\n",
      "training epoch  12 ...      4832.704852104187\n",
      "training epoch  13 ...      4786.04335641861\n",
      "training epoch  14 ...      4744.084822177887\n",
      "training epoch  15 ...      4705.929119110107\n",
      "post training loss:  9.177081535028856\n",
      "finished.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "pre training loss:  9.97839768978493\n",
      "training dataset: products-10k\n",
      "training epoch  1 ...      27628.150186538696\n",
      "training epoch  2 ...      19033.52380180359\n",
      "training epoch  3 ...      17150.3316988945\n",
      "training epoch  4 ...      16277.077817440033\n",
      "training epoch  5 ...      15738.200339317322\n",
      "post training loss:  12.69208487528959\n",
      "finished.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "pre training loss:  10.956452699465173\n",
      "training dataset: shopee\n",
      "training epoch  1 ...      8390.82402420044\n",
      "training epoch  2 ...      4928.819581031799\n",
      "training epoch  3 ...      3271.7346873283386\n",
      "training epoch  4 ...      2388.1270849704742\n",
      "training epoch  5 ...      1913.6574697494507\n",
      "training epoch  6 ...      1643.0231442451477\n",
      "training epoch  7 ...      1475.6574025154114\n",
      "training epoch  8 ...      1365.8734773397446\n",
      "training epoch  9 ...      1288.0434571504593\n",
      "training epoch  10 ...      1227.5690114498138\n",
      "post training loss:  12.609829840259017\n",
      "finished.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "pre training loss:  9.017093590327672\n",
      "training dataset: imagenet1000\n",
      "training epoch  1 ...      6690.697525024414\n",
      "training epoch  2 ...      4622.480632781982\n",
      "training epoch  3 ...      3955.2700476646423\n",
      "training epoch  4 ...      3671.4457869529724\n",
      "training epoch  5 ...      3512.3797421455383\n",
      "training epoch  6 ...      3405.199257850647\n",
      "training epoch  7 ...      3324.938853740692\n",
      "training epoch  8 ...      3260.968530654907\n",
      "training epoch  9 ...      3207.908146381378\n",
      "training epoch  10 ...      3162.67280626297\n",
      "post training loss:  11.318316299245105\n",
      "finished.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "pre training loss:  8.673189309415918\n",
      "training dataset: places\n",
      "training epoch  1 ...      4177.236164093018\n",
      "training epoch  2 ...      3312.0289602279663\n",
      "post training loss:  9.442983168758166\n",
      "finished.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "pre training loss:  9.520006542685646\n",
      "training dataset: imagenet-sketch\n",
      "training epoch  1 ...      8902.399318695068\n",
      "training epoch  2 ...      6348.020544052124\n",
      "post training loss:  9.91529888177068\n",
      "finished.\n",
      "==================================================\n",
      "\n",
      "architecture-dataset  :  0.7722986936569214   +0.042845189571380615\n",
      "GPR                   :  0.8054666519165039   +0.12883329391479492\n",
      "food101               :  0.8609066009521484   +0.12823337316513062\n",
      "fruits360             :  0.9955152273178101   +0.0011092424392700195\n",
      "rp2k                  :  0.6035001873970032   +0.17380154132843018\n",
      "products-10k          :  0.3372558653354645   +0.16292113065719604\n",
      "shopee                :  0.41354745626449585   +0.0711824893951416\n",
      "imagenet1000          :  0.7062138319015503   +0.2588516175746918\n",
      "places                :  0.537222146987915   +0.1288517713546753\n",
      "imagenet-sketch       :  0.7114307284355164   +0.16483718156814575\n",
      "imagenet1000          :  0.7063923478126526   +0.25909343361854553\n"
     ]
    }
   ],
   "source": [
    "def train_arc_ds( model, ds, ds_config):    \n",
    "    \n",
    "    #optim = torch.optim.Adam(model.parameters(), lr=ds_config.lr)\n",
    "    base_optimizer = torch.optim.Adam\n",
    "    optimizer = SAM(model.parameters(), base_optimizer, lr=0.001)  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(ds_config.epochs):\n",
    "        print('training epoch ',epoch+1,'...',end='')\n",
    "        total_loss = 0\n",
    "        for i, batch in enumerate(ds):\n",
    "            _input, label = batch\n",
    "            \n",
    "            image_preds = model.with_arc(_input.to(device),label.to(device))   #output = model(input)\n",
    "            #print(image_preds.shape, exam_pred.shape)\n",
    "\n",
    "            loss = criterion(image_preds, label.to(device)) \n",
    "            loss.backward()\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "\n",
    "            # second forward-backward pass\n",
    "            criterion(model.with_arc(_input.to(device), label.to(device)), label.to(device)).backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "            total_loss += loss.detach().item()\n",
    "\n",
    "            #_input = _input.to(device)\n",
    "            #optim.zero_grad()\n",
    "            #output = model.with_arc( _input, label)\n",
    "\n",
    "            #loss = criterion( output, label.to(device))\n",
    "            #loss.backward()\n",
    "            #optim.step()\n",
    "        print('      {}'.format(total_loss))\n",
    "        total_loss = 0\n",
    "    return model\n",
    "if True:\n",
    "    get_retrieval_scores()\n",
    "    for ds_config in config.class_data_ls:\n",
    "        if ds_config.train:\n",
    "            print('='*50)\n",
    "            ds = classification_dataset(ds_config)\n",
    "            ds = ds.train_split()\n",
    "            model.lid = nn.Linear(64, ds_config.max_class).to(device)\n",
    "            model.arc = ArcMarginProduct( ds_config.max_class,ds_config.max_class).to(device)\n",
    "            print('pre training loss: ',eval_classification_ds( model, ds, ds_config))\n",
    "\n",
    "            print('training dataset: {}'.format(ds_config.path.split('/')[-1]))\n",
    "            model = train_arc_ds( model, ds, ds_config)\n",
    "            print('post training loss: ',eval_classification_ds( model, ds, ds_config))\n",
    "            #get_retrieval_scores()\n",
    "            print('finished.')\n",
    "            print('='*50)\n",
    "            print('')\n",
    "    get_retrieval_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a830ce3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T20:04:45.454530Z",
     "iopub.status.busy": "2022-09-16T20:04:45.454199Z",
     "iopub.status.idle": "2022-09-16T20:04:45.461990Z",
     "shell.execute_reply": "2022-09-16T20:04:45.461088Z"
    },
    "papermill": {
     "duration": 0.022136,
     "end_time": "2022-09-16T20:04:45.463912",
     "exception": false,
     "start_time": "2022-09-16T20:04:45.441776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, m=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()  # pre 3.3 syntax\n",
    "        self.m = m  # margin or radius\n",
    "\n",
    "    def forward(self, y1, y2, d=0):\n",
    "        # d = 0 means y1 and y2 are supposed to be same\n",
    "        # d = 1 means y1 and y2 are supposed to be different\n",
    "\n",
    "        euc_dist = T.nn.functional.pairwise_distance(y1, y2)\n",
    "\n",
    "        if d == 0:\n",
    "            return T.mean(T.pow(euc_dist, 2))  # distance squared\n",
    "        else:  # d == 1\n",
    "            delta = self.m - euc_dist  # sort of reverse distance\n",
    "            delta = T.clamp(delta, min=0.0, max=None)\n",
    "            return T.mean(T.pow(delta, 2))  # mean over all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72d442b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T20:04:45.487366Z",
     "iopub.status.busy": "2022-09-16T20:04:45.487104Z",
     "iopub.status.idle": "2022-09-16T20:04:45.495858Z",
     "shell.execute_reply": "2022-09-16T20:04:45.494963Z"
    },
    "papermill": {
     "duration": 0.022599,
     "end_time": "2022-09-16T20:04:45.497856",
     "exception": false,
     "start_time": "2022-09-16T20:04:45.475257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_encoding_class_ds(model, ds, ds_config):\n",
    "    optim = torch.optim.SGD(model.fw.parameters(), lr=ds_config.lr)\n",
    "    criterion = nn.MSELoss()#ds_config.criterion\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    for i, batch in enumerate(ds):\n",
    "        _input, label = batch\n",
    "        optim.zero_grad()\n",
    "        model.zero_grad()\n",
    "        output = model(_input.to(device))\n",
    "        loss = criterion( output, _input.to(device)) \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_loss += loss.detach().item()\n",
    "    print('     {}'.format(total_loss / len(ds)))\n",
    "    return model\n",
    "\n",
    "if False:\n",
    "    get_retrieval_scores()\n",
    "    model.lid = nn.Linear(64,768).to(device)\n",
    "    for epoch in range(10):\n",
    "        for ds_config in config.class_data_ls:\n",
    "            if ds_config.train:\n",
    "                ds = classification_dataset(ds_config)\n",
    "                print('training ',ds_config.path)\n",
    "                print('epoch {} training {}...'.format( epoch+1, ds_config.path.split('/')[-1]),end='')\n",
    "                model = train_encoding_class_ds( model, ds, ds_config)\n",
    "\n",
    "                print('')\n",
    "    get_retrieval_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e9f4e42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T20:04:45.521831Z",
     "iopub.status.busy": "2022-09-16T20:04:45.521072Z",
     "iopub.status.idle": "2022-09-16T20:04:45.525644Z",
     "shell.execute_reply": "2022-09-16T20:04:45.524800Z"
    },
    "papermill": {
     "duration": 0.018602,
     "end_time": "2022-09-16T20:04:45.527554",
     "exception": false,
     "start_time": "2022-09-16T20:04:45.508952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_margin(model, ds):\n",
    "    pass\n",
    "    #this should evaluate the embedding difference between model outputs of similar and different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "894df76f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T20:04:45.551292Z",
     "iopub.status.busy": "2022-09-16T20:04:45.550556Z",
     "iopub.status.idle": "2022-09-16T20:04:45.560192Z",
     "shell.execute_reply": "2022-09-16T20:04:45.559315Z"
    },
    "papermill": {
     "duration": 0.023469,
     "end_time": "2022-09-16T20:04:45.562131",
     "exception": false,
     "start_time": "2022-09-16T20:04:45.538662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#idea, have a special arcface layer to go on top of the flattened covariance matrix,\n",
    "#it will just have class 0 and 1\n",
    "#input: 64^2, output: 64^2\n",
    "\n",
    "def train_similarities( model, ds, ds_config):\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.00005)#ds_config.lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(ds_config.epochs):\n",
    "        print('training epoch ',epoch+1,'...',end='')\n",
    "        total_loss = 0\n",
    "        for i, batch in enumerate(ds):\n",
    "            _input, label = batch\n",
    "            \n",
    "            _input = _input.to(device)\n",
    "            optim.zero_grad()\n",
    "            output = model( _input, use_lid=False)\n",
    "            #output = model.arc_sim( _input, label.to(device))\n",
    "            x, y = torch.meshgrid(label,label)\n",
    "            mesh = (x==y).type(torch.uint8).type(torch.float)\n",
    "            covariance = output @ output.T\n",
    "            loss = criterion( covariance, mesh)\n",
    "            total_loss += loss.detach().item()\n",
    "            \n",
    "            #loss = criterion( output, label.to(device))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        print('   loss: {}'.format(total_loss / len(ds)))\n",
    "    return model\n",
    "if False:\n",
    "    print('training similarities... ')\n",
    "    #get_retrieval_scores()\n",
    "    model.arc = ArcMarginProduct(64,64).to(device)\n",
    "    for ds_config in config.class_data_ls:\n",
    "        if ds_config.train:\n",
    "            ds = classification_dataset(ds_config)\n",
    "            print('training ',ds_config.path)\n",
    "            \n",
    "            model = train_similarities( model, ds, ds_config)\n",
    "            print('finished.')\n",
    "    get_retrieval_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c55fd7d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T20:04:45.585892Z",
     "iopub.status.busy": "2022-09-16T20:04:45.585138Z",
     "iopub.status.idle": "2022-09-16T20:04:45.593956Z",
     "shell.execute_reply": "2022-09-16T20:04:45.593118Z"
    },
    "papermill": {
     "duration": 0.022705,
     "end_time": "2022-09-16T20:04:45.595904",
     "exception": false,
     "start_time": "2022-09-16T20:04:45.573199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class embedding_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.vecs = ['vec/'+_dir for _dir in os.listdir(os.path.join(config.path, 'vec'))]\n",
    "        self.vecs.sort()\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.vecs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vec = torch.load(os.path.join(self.config.path,self.vecs[idx]),map_location=device)\n",
    "        return vec\n",
    "\n",
    "def train_embedding_ds( model, ds, ds_config):\n",
    "    optim = torch.optim.SGD(model.fw.parameters(), lr=ds_config.lr)\n",
    "    criterion = ds_config.criterion\n",
    "    model.train()\n",
    "    for i, batch in enumerate(ds):\n",
    "        optim.zero_grad()\n",
    "        model.zero_grad()\n",
    "\n",
    "        output = model(batch.to(device), use_lid=False)\n",
    "        label_vec = torch.sum( output, 0) / batch.shape[0] \n",
    "\n",
    "        loss = criterion( output, label_vec[None,:].repeat(batch.shape[0], 1))# * 10 \n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8adc2e25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T20:04:45.620059Z",
     "iopub.status.busy": "2022-09-16T20:04:45.619199Z",
     "iopub.status.idle": "2022-09-16T20:05:41.762780Z",
     "shell.execute_reply": "2022-09-16T20:05:41.760952Z"
    },
    "papermill": {
     "duration": 56.158109,
     "end_time": "2022-09-16T20:05:41.764967",
     "exception": false,
     "start_time": "2022-09-16T20:04:45.606858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture-dataset  :  0.7722986936569214   +0.0\n",
      "GPR                   :  0.8054666519165039   +0.0\n",
      "food101               :  0.8609066009521484   +0.0\n",
      "fruits360             :  0.9955152273178101   +0.0\n",
      "rp2k                  :  0.6035001873970032   +0.0\n",
      "products-10k          :  0.3372558653354645   +0.0\n",
      "shopee                :  0.41354745626449585   +0.0\n",
      "imagenet1000          :  0.7062138319015503   +0.0\n",
      "places                :  0.537222146987915   +0.0\n",
      "imagenet-sketch       :  0.7114307284355164   +0.0\n",
      "imagenet1000          :  0.7063923478126526   +0.0\n",
      "training  /kaggle/input/notebook-data/embedding/imagenet1000\n",
      "1 / 3 training /kaggle/input/notebook-data/embedding/imagenet1000...     6.631022381852586\n",
      "2 / 3 training /kaggle/input/notebook-data/embedding/imagenet1000...     6.471067062904739\n",
      "3 / 3 training /kaggle/input/notebook-data/embedding/imagenet1000...     6.388194638531542\n",
      "\n",
      "training  /kaggle/input/notebook-data/embedding/google-landmarks-2021-V1\n",
      "1 / 2 training /kaggle/input/notebook-data/embedding/google-landmarks-2021-V1...     4.5519337413728405\n",
      "2 / 2 training /kaggle/input/notebook-data/embedding/google-landmarks-2021-V1...     4.081547492802635\n",
      "\n",
      "training  /kaggle/input/notebook-data/embedding/fashion\n",
      "1 / 2 training /kaggle/input/notebook-data/embedding/fashion...     6.8614621226852\n",
      "2 / 2 training /kaggle/input/notebook-data/embedding/fashion...     6.763051094235601\n",
      "\n",
      "training  /kaggle/input/notebook-data/embedding/GPR12000\n",
      "1 / 2 training /kaggle/input/notebook-data/embedding/GPR12000...     5.69517835021019\n",
      "2 / 2 training /kaggle/input/notebook-data/embedding/GPR12000...     5.458251958092053\n",
      "\n",
      "architecture-dataset  :  0.7728827595710754   +0.0005840659141540527\n",
      "GPR                   :  0.8057000041007996   +0.0002333521842956543\n",
      "food101               :  0.8614171147346497   +0.0005105137825012207\n",
      "fruits360             :  0.9954795837402344    -3.5643577575683594e-05\n",
      "rp2k                  :  0.6046050786972046   +0.001104891300201416\n",
      "products-10k          :  0.3368774652481079    -0.0003784000873565674\n",
      "shopee                :  0.4135824739933014   +3.501772880554199e-05\n",
      "imagenet1000          :  0.7070543169975281   +0.0008404850959777832\n",
      "places                :  0.5381288528442383   +0.0009067058563232422\n",
      "imagenet-sketch       :  0.7107980251312256    -0.0006327033042907715\n",
      "imagenet1000          :  0.7069391012191772   +0.0005467534065246582\n"
     ]
    }
   ],
   "source": [
    "def train_encoding_ds(model, ds, ds_config):\n",
    "    optim = torch.optim.SGD(model.fw.parameters(), lr=ds_config.lr)\n",
    "    criterion = nn.MSELoss()#ds_config.criterion\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    for i, batch in enumerate(ds):\n",
    "        optim.zero_grad()\n",
    "        model.zero_grad()\n",
    "        output = model(batch.to(device))\n",
    "        loss = criterion( output, batch) \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_loss += loss.detach().item()\n",
    "    print('     {}'.format(total_loss / len(ds)))\n",
    "    return model\n",
    "get_retrieval_scores()\n",
    "for ds_config in config.embed_data_ls:\n",
    "    if ds_config.train:\n",
    "        model.lid = nn.Linear(64,768).to(device)\n",
    "        ds = embedding_dataset(ds_config)\n",
    "        print('training ',ds_config.path)\n",
    "        \n",
    "        for epoch in range(ds_config.epochs):\n",
    "            print('{} / {} training {}...'.format( epoch+1, ds_config.epochs, ds_config.path),end='')\n",
    "            model = train_encoding_ds( model, ds, ds_config)\n",
    "        \n",
    "        print('')\n",
    "get_retrieval_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa4221af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T20:05:41.792222Z",
     "iopub.status.busy": "2022-09-16T20:05:41.791309Z",
     "iopub.status.idle": "2022-09-16T20:05:41.798254Z",
     "shell.execute_reply": "2022-09-16T20:05:41.797401Z"
    },
    "papermill": {
     "duration": 0.022232,
     "end_time": "2022-09-16T20:05:41.800022",
     "exception": false,
     "start_time": "2022-09-16T20:05:41.777790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_embedding_ds( model, ds, ds_config):\n",
    "    \n",
    "    criterion = ds_config.criterion\n",
    "    model.eval()\n",
    "    total_loss=0\n",
    "    for i, batch in enumerate(ds):\n",
    "        with torch.no_grad():\n",
    "            output = model(batch.to(device), use_lid=False)\n",
    "        label_vec = torch.sum( output, 0) / batch.shape[0] \n",
    "        loss = criterion( output, label_vec[None,:].repeat(batch.shape[0], 1)).detach().item()# * 10 \n",
    "        total_loss+=loss\n",
    "    return total_loss / len(ds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8e23fad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T20:05:41.827198Z",
     "iopub.status.busy": "2022-09-16T20:05:41.826332Z",
     "iopub.status.idle": "2022-09-16T20:05:41.831030Z",
     "shell.execute_reply": "2022-09-16T20:05:41.830111Z"
    },
    "papermill": {
     "duration": 0.02024,
     "end_time": "2022-09-16T20:05:41.833072",
     "exception": false,
     "start_time": "2022-09-16T20:05:41.812832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for ds_config in config.embed_data_ls:\n",
    "#    if ds_config.train[0]:\n",
    "#        ds = embedding_dataset(ds_config)\n",
    "#       print('training ',ds_config.path)\n",
    "#       print('pre training loss: ',eval_embedding_ds( model, ds, ds_config))\n",
    "#        get_retrieval_scores()\n",
    "#       for epoch in range(ds_config.epochs):\n",
    "#           print('{} / {} training {}...'.format( epoch+1, ds_config.epochs, ds_config.path))\n",
    "#            model = train_embedding_ds( model, ds, ds_config)\n",
    "#            get_retrieval_scores()\n",
    "#        print('post training loss: ',eval_embedding_ds( model, ds, ds_config))\n",
    "#        get_retrieval_scores()\n",
    "#        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f37a5942",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T20:05:41.859980Z",
     "iopub.status.busy": "2022-09-16T20:05:41.859716Z",
     "iopub.status.idle": "2022-09-16T20:05:41.869942Z",
     "shell.execute_reply": "2022-09-16T20:05:41.869089Z"
    },
    "papermill": {
     "duration": 0.025983,
     "end_time": "2022-09-16T20:05:41.871890",
     "exception": false,
     "start_time": "2022-09-16T20:05:41.845907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_embedding_lib( model, ds, ds_config):    \n",
    "    #1. find each class in the dataset\n",
    "    all_labels = None\n",
    "    for i, batch in enumerate(ds):\n",
    "        _, label = batch\n",
    "        if all_labels is None:\n",
    "            all_labels = label\n",
    "        else:\n",
    "            all_labels = torch.concat((all_labels, label),0)\n",
    "    unique = torch.unique(all_labels)\n",
    "    \n",
    "    #2. for each class, gather all of the vectors for that class\n",
    "    vecs = None\n",
    "    for label in unique:\n",
    "        print('-',end='')\n",
    "        class_vecs = None\n",
    "        for i, batch in enumerate(ds):\n",
    "            vec, labels = batch\n",
    "            mask = (labels == label).nonzero()#torch.where(labels==label, label, -1)\n",
    "            selected = vec[mask]\n",
    "            if selected.shape[0] is not 0:\n",
    "                \n",
    "                if class_vecs is None:\n",
    "                    class_vecs = selected\n",
    "                else:\n",
    "                    class_vecs = torch.concat((class_vecs, selected),0)    \n",
    "                    \n",
    "        #3. for each set of vectors run the model on each vector, and get the average vector\n",
    "        class_vecs = class_vecs.squeeze()\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            class_vecs,\n",
    "            batch_size=64\n",
    "        )\n",
    "        output_vecs = None\n",
    "        for i, batch in enumerate(loader):\n",
    "            output = model(batch, use_lid=False)\n",
    "            if output_vecs is None:\n",
    "                output_vecs = output\n",
    "            else:\n",
    "                output_vecs = torch.concat((output_vecs, output),0)\n",
    "                \n",
    "        #4. calculate an average output vector for each class\n",
    "        mean_vec = torch.sum( output_vecs, 0) / class_vecs.shape[0]\n",
    "        mean_vec = mean_vec[None, :]\n",
    "        if vecs is None:\n",
    "            vecs = mean_vec\n",
    "        else:\n",
    "            vecs = torch.concat(( vecs, mean_vec), 0)\n",
    "    print(vecs.shape)\n",
    "    return vecs\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "#for ds_config in config.class_data_ls[:1]:\n",
    "#    if ds_config.train:\n",
    "#        ds = classification_dataset(ds_config)\n",
    "#        sample_vec_lib = make_embedding_lib( model, ds, ds_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b28d9",
   "metadata": {
    "papermill": {
     "duration": 0.012177,
     "end_time": "2022-09-16T20:05:41.897333",
     "exception": false,
     "start_time": "2022-09-16T20:05:41.885156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a6b4565",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T20:05:41.923510Z",
     "iopub.status.busy": "2022-09-16T20:05:41.923213Z",
     "iopub.status.idle": "2022-09-16T20:05:41.929438Z",
     "shell.execute_reply": "2022-09-16T20:05:41.928437Z"
    },
    "papermill": {
     "duration": 0.021526,
     "end_time": "2022-09-16T20:05:41.931458",
     "exception": false,
     "start_time": "2022-09-16T20:05:41.909932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class JSD(nn.Module):\n",
    "    #thanks to:\n",
    "    #https://discuss.pytorch.org/t/jensen-shannon-divergence/2626/12\n",
    "    def __init__(self):\n",
    "        super(JSD, self).__init__()\n",
    "        self.kl = nn.KLDivLoss(reduction='batchmean', log_target=True)\n",
    "\n",
    "    def forward(self, p: torch.tensor, q: torch.tensor):\n",
    "        p, q = p.view(-1, p.size(-1)).log_softmax(-1), q.view(-1, q.size(-1)).log_softmax(-1)\n",
    "        m = (0.5 * (p + q))\n",
    "        return 0.5 * (self.kl(m, p) + self.kl(m, q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c84c5f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T20:05:41.957450Z",
     "iopub.status.busy": "2022-09-16T20:05:41.957171Z",
     "iopub.status.idle": "2022-09-16T20:05:41.964310Z",
     "shell.execute_reply": "2022-09-16T20:05:41.963272Z"
    },
    "papermill": {
     "duration": 0.022695,
     "end_time": "2022-09-16T20:05:41.966587",
     "exception": false,
     "start_time": "2022-09-16T20:05:41.943892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp = model\n",
    "def train_lookup_embed_ds( model, ds, ds_config, vec_lib):\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=ds_config.lr)\n",
    "    criterion = JSD()\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(5):#ds_config.epochs):\n",
    "        print('training epoch ',epoch+1,'...')\n",
    "        for i, batch in enumerate(ds):\n",
    "            _input, label = batch\n",
    "            \n",
    "            _input = _input.to(device)\n",
    "            optim.zero_grad()\n",
    "            model.zero_grad()\n",
    "            output = model(_input,use_lid=False)\n",
    "            \n",
    "            label = vec_lib[label]\n",
    "            loss = criterion( output, label.detach().to(device))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "    return model\n",
    "#for ds_config in config.class_data_ls:\n",
    "#    if ds_config.train:\n",
    "#        ds = classification_dataset(ds_config)\n",
    "#        print('training ',ds_config.path)\n",
    "#        temp = train_lookup_embed_ds( temp, ds, ds_config, sample_vec_lib)\n",
    "#        print('finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "265163a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T20:05:41.993626Z",
     "iopub.status.busy": "2022-09-16T20:05:41.993327Z",
     "iopub.status.idle": "2022-09-16T20:05:42.001179Z",
     "shell.execute_reply": "2022-09-16T20:05:42.000350Z"
    },
    "papermill": {
     "duration": 0.024005,
     "end_time": "2022-09-16T20:05:42.003173",
     "exception": false,
     "start_time": "2022-09-16T20:05:41.979168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LookupModel(nn.Module):\n",
    "    def __init__(self, _model):\n",
    "        super().__init__()\n",
    "        self._model = _model\n",
    "        self.vec_lib = None\n",
    "    def forward( self, x, use_lid=False):\n",
    "        output = self._model(x,use_lid=False)[:,None,:]\n",
    "        output = output.repeat(1,self.vec_lib.shape[0],1)\n",
    "        vec_lib = self.vec_lib.repeat(output.shape[0],1,1)\n",
    "        product = output @ vec_lib.transpose(1,2)\n",
    "        product = product[:,:1,:]\n",
    "        value, index = torch.topk(product.squeeze(), k=1,dim=1)\n",
    "        return self.vec_lib[index.squeeze()]\n",
    "#m = LookupModel(model)\n",
    "#m.vec_lib = sample_vec_lib\n",
    "#m(torch.zeros(3,768)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "875714f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T20:05:42.029933Z",
     "iopub.status.busy": "2022-09-16T20:05:42.029673Z",
     "iopub.status.idle": "2022-09-16T20:05:42.033716Z",
     "shell.execute_reply": "2022-09-16T20:05:42.032674Z"
    },
    "papermill": {
     "duration": 0.019938,
     "end_time": "2022-09-16T20:05:42.035926",
     "exception": false,
     "start_time": "2022-09-16T20:05:42.015988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model(torch.zeros(3,768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ff3dbfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T20:05:42.061899Z",
     "iopub.status.busy": "2022-09-16T20:05:42.061643Z",
     "iopub.status.idle": "2022-09-16T20:05:42.065548Z",
     "shell.execute_reply": "2022-09-16T20:05:42.064571Z"
    },
    "papermill": {
     "duration": 0.019201,
     "end_time": "2022-09-16T20:05:42.067579",
     "exception": false,
     "start_time": "2022-09-16T20:05:42.048378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#temp(torch.zeros(3,768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00692afc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T20:05:42.094104Z",
     "iopub.status.busy": "2022-09-16T20:05:42.093824Z",
     "iopub.status.idle": "2022-09-16T20:05:42.097787Z",
     "shell.execute_reply": "2022-09-16T20:05:42.096847Z"
    },
    "papermill": {
     "duration": 0.019556,
     "end_time": "2022-09-16T20:05:42.099678",
     "exception": false,
     "start_time": "2022-09-16T20:05:42.080122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#_eval(model)\n",
    "#_eval(m)\n",
    "#trained_temp = LookupModel(temp)\n",
    "#trained_temp.vec_lib = sample_vec_lib\n",
    "#_eval(trained_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb66ef39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T20:05:42.126256Z",
     "iopub.status.busy": "2022-09-16T20:05:42.125339Z",
     "iopub.status.idle": "2022-09-16T20:06:25.737529Z",
     "shell.execute_reply": "2022-09-16T20:06:25.736321Z"
    },
    "papermill": {
     "duration": 43.627634,
     "end_time": "2022-09-16T20:06:25.739702",
     "exception": false,
     "start_time": "2022-09-16T20:05:42.112068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ftfy\r\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\r\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m670.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (2021.11.10)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.64.0)\r\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from ftfy) (0.2.5)\r\n",
      "Installing collected packages: ftfy\r\n",
      "Successfully installed ftfy-6.1.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCollecting git+https://github.com/openai/CLIP.git\r\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-_6h4rjl8\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-_6h4rjl8\r\n",
      "  Resolved https://github.com/openai/CLIP.git to commit d50d76daa670286dd6cacf3bcd80b5e4823fc8e1\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (6.1.1)\r\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (2021.11.10)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (4.64.0)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (1.11.0)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (0.12.0)\r\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from ftfy->clip==1.0) (0.2.5)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->clip==1.0) (4.3.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision->clip==1.0) (2.28.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->clip==1.0) (1.21.6)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->clip==1.0) (9.1.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->clip==1.0) (2022.6.15)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->clip==1.0) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->clip==1.0) (1.26.12)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->clip==1.0) (2.1.0)\r\n",
      "Building wheels for collected packages: clip\r\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369409 sha256=980b176f29c705675b77647b70cdd6212d7afafb03baa4840a92d8c89fe0212b\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1wrcxmkw/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\r\n",
      "Successfully built clip\r\n",
      "Installing collected packages: clip\r\n",
      "Successfully installed clip-1.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 891M/891M [00:03<00:00, 245MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening:  /root/.cache/clip/ViT-L-14-336px.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py:1110: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:601.)\n",
      "  return forward_call(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9873, -7.2148, -2.6152,  7.3945, -1.9160,  3.2363, -2.2305, -4.0938,\n",
      "         -1.5557,  1.3984,  1.6836,  3.2324, -5.2461,  0.1132, -4.1406, -4.7656,\n",
      "         -0.2913,  2.2266,  3.8047, -0.4146, -2.6523, -2.4512, -2.6992,  1.2139,\n",
      "         -2.3438,  1.5293, -2.9336, -0.7905, -3.1309, -0.7441, -2.2168,  0.8643,\n",
      "         -2.3926,  1.1592, -0.1013, -4.0586,  1.6426,  2.7227,  2.6777,  0.4241,\n",
      "          4.9297, -2.3809, -1.7236,  0.9590,  1.2266,  2.7754, -3.4023, -0.1344,\n",
      "          0.6519, -1.4082,  3.6172,  0.6284, -1.3262,  3.9141,  2.5703,  6.6797,\n",
      "         -2.7188,  4.3242, -0.4131, -2.0098, -6.7227,  1.3379,  0.9478,  0.0483]],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5.473  , -4.97   , -2.479  ,  1.81   , -2.357  ,  3.297  ,\n",
       "        -1.745  , -4.375  , -1.862  ,  2.08   , -0.1088 ,  1.363  ,\n",
       "        -0.879  ,  3.889  , -0.2479 , -1.391  , -1.278  ,  2.8    ,\n",
       "         4.906  , -2.104  , -3.885  ,  1.708  , -3.998  , -0.6377 ,\n",
       "        -0.807  ,  1.625  , -0.741  , -3.172  , -0.678  ,  1.252  ,\n",
       "         0.2866 ,  2.244  , -2.107  , -0.859  , -2.633  , -2.094  ,\n",
       "         1.699  , -0.235  ,  0.4277 ,  0.06097,  2.965  , -1.166  ,\n",
       "        -0.479  ,  3.441  ,  2.436  ,  2.23   , -4.184  , -0.3992 ,\n",
       "         3.834  , -0.51   , -0.977  ,  0.8135 ,  0.5063 ,  2.121  ,\n",
       "         3.283  ,  1.277  ,  0.606  ,  2.846  , -0.2996 , -1.479  ,\n",
       "        -2.742  , -0.4424 , -1.549  , -0.528  ]], dtype=float16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "from clip.clip import _download, _MODELS\n",
    "\n",
    "\n",
    "model_path = _download(_MODELS['ViT-L/14@336px'], os.path.expanduser(\"~/.cache/clip\"))\n",
    "with open(model_path, 'rb') as opened_file:\n",
    "    print('opening: ',model_path)\n",
    "    clip_vit_l14_336 = torch.jit.load(opened_file, map_location=device).visual.eval()\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = clip_vit_l14_336\n",
    "        \n",
    "        self.fw = None\n",
    "        self.pool = nn.AdaptiveAvgPool1d(64)\n",
    "    def preprocess_image(self, x):\n",
    "        x = transforms.functional.resize(x,size=[336, 336])\n",
    "        x = x/255.0\n",
    "        x = transforms.functional.normalize(x, \n",
    "                                            mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                                            std=[0.26862954, 0.26130258, 0.27577711])\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.preprocess_image(x)\n",
    "        x = self.encoder(x.half())\n",
    "        x = self.fw(x)\n",
    "        #x = torch.nn.functional.normalize(x, p=2.0, dim=1, eps=1e-12)\n",
    "        return x\n",
    "\n",
    "sub = MyModel().to(device).eval()\n",
    "sub.fw = model.fw.half()\n",
    "#print(sub)\n",
    "print(sub(torch.randn(1,3,336,336).to(device)).detach())\n",
    "\n",
    "sub.eval()\n",
    "saved_model = torch.jit.script(sub)\n",
    "saved_model.save(\"saved_model.pt\")\n",
    "with ZipFile('submission.zip','w') as zip:           \n",
    "    zip.write(\"saved_model.pt\", arcname='saved_model.pt')\n",
    "sub = torch.jit.load(\"saved_model.pt\").to('cuda').eval()\n",
    "input_batch = torch.rand(1, 3, 336, 336).to('cuda')\n",
    "with torch.no_grad():\n",
    "    embedding = sub(input_batch).cpu().data.numpy()\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f83b422",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T20:06:25.773590Z",
     "iopub.status.busy": "2022-09-16T20:06:25.772596Z",
     "iopub.status.idle": "2022-09-16T20:06:25.779196Z",
     "shell.execute_reply": "2022-09-16T20:06:25.778176Z"
    },
    "papermill": {
     "duration": 0.025781,
     "end_time": "2022-09-16T20:06:25.781556",
     "exception": false,
     "start_time": "2022-09-16T20:06:25.755775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=768, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8169c9a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T20:06:25.814168Z",
     "iopub.status.busy": "2022-09-16T20:06:25.813890Z",
     "iopub.status.idle": "2022-09-16T20:06:25.822568Z",
     "shell.execute_reply": "2022-09-16T20:06:25.821720Z"
    },
    "papermill": {
     "duration": 0.027283,
     "end_time": "2022-09-16T20:06:25.824497",
     "exception": false,
     "start_time": "2022-09-16T20:06:25.797214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_embed_ds(path_ds, model):\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        path_ds,\n",
    "        batch_size=config.batch_size,\n",
    "    )\n",
    "    \n",
    "    mse = torch.nn.MSELoss()\n",
    "    \n",
    "    #optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.000005, amsgrad=True)\n",
    "    optim = torch.optim.SGD(model.fw.parameters(), lr=config.embed_lr)\n",
    "    \n",
    "    for i, batch in enumerate(loader):\n",
    "        optim.zero_grad()\n",
    "        model.zero_grad()\n",
    "        model.train()\n",
    "        output = model(batch.to(device))\n",
    "        #print('ran model')\n",
    "        \n",
    "        \n",
    "        label_vec = torch.sum( output, 0) / batch.shape[0] # calculate the average output vector\n",
    "        # essentially, make the outputs of the model more similar to each other for each class\n",
    "        # 'tighen' the vector output for the distribution of samples\n",
    "        loss = mse( output, label_vec[None,:].repeat(batch.shape[0], 1)) * 10 \n",
    "        print(loss.item())\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        #print(model.fw[0].weight)\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # I have no idea what this does\n",
    "        optim.step()\n",
    "        #print(model.fw[0].weight)\n",
    "\n",
    "\n",
    "if False:#config.train_class:\n",
    "    \n",
    "    top_fw = nn.Sequential(\n",
    "            nn.Linear(768, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 256),\n",
    "    ).to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bccd8575",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T20:06:25.857522Z",
     "iopub.status.busy": "2022-09-16T20:06:25.856736Z",
     "iopub.status.idle": "2022-09-16T20:06:25.864374Z",
     "shell.execute_reply": "2022-09-16T20:06:25.863584Z"
    },
    "papermill": {
     "duration": 0.026,
     "end_time": "2022-09-16T20:06:25.866275",
     "exception": false,
     "start_time": "2022-09-16T20:06:25.840275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:#config.train_embed:\n",
    "    \n",
    "    #train on imagenet classes\n",
    "    #path = '/kaggle/input/imagenetmini-1000/imagenet-mini/train'\n",
    "    #classes = os.listdir(path)\n",
    "    #idx=0\n",
    "    #for _class in classes[:config.imagenet_classes]:\n",
    "    #    idx+=1\n",
    "    #    print(' imagenetmini1000 | trainig class {} | {} / {} ...'.format( _class, idx, config.imagenet_classes))\n",
    "    #    path_ds = load_imagenet_class(os.path.join(path, _class))\n",
    "    #    train_class( path_ds, model)\n",
    "    idx=0 \n",
    "    for _class in get_imagenet1000_classes()[:config.imagenet_embed_classes]:\n",
    "        idx+=1\n",
    "        print(' imagenet1000 | training embed class {} | {} / {} ...'.format( _class, idx, 200))\n",
    "        path_ds = load_imagenet1000_class(_class)\n",
    "        train_embed_ds( path_ds, model)\n",
    "    #train on caltech256 classes\n",
    "    idx=0\n",
    "    for _class in get_caltech256_classes()[:config.caltech256_embed_classes]:\n",
    "        idx+=1\n",
    "        print(' caltech256 | training embed class {} | {} / {} ...'.format( _class, idx, 20))\n",
    "        path_ds = load_caltech256_class(_class)\n",
    "        train_embed_ds( path_ds, model)\n",
    "    idx=0\n",
    "    for _class in get_fashion_classes()[:config.fashion_embed_classes]:\n",
    "        idx+=1\n",
    "        print(' caltech256 | training embed class {} | {} / {} ...'.format( _class, idx, config.fasion_embed_classes))\n",
    "        path_ds = load_fashion_class(_class)\n",
    "        train_embed_ds( path_ds, model)\n",
    "        \n",
    "        \n",
    "    \n",
    "    #train on google landmark recognition 2021 classes\n",
    "    #idx=0\n",
    "    #for _class in get_google_landmarks_2021_classes()[:500]:\n",
    "    #    idx+=1\n",
    "    #    print(' Google Landmarks 2021 | training class {} | {} / {}'.format( _class, idx, 500))\n",
    "    #    path_ds = load_google_landmarks_2021_class(_class)\n",
    "    #    train_class( path_ds, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1392.828317,
   "end_time": "2022-09-16T20:06:28.390424",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-09-16T19:43:15.562107",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
